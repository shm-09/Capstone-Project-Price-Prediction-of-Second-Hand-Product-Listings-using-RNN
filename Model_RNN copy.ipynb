{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "start_real = datetime.now()\n",
    "\n",
    "\n",
    "# from keras.layers import Bidirectional\n",
    "# set seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining RMSLE Error Functions #\n",
    "This will be used to check model accuracy after preductions are made on the train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(Y, Y_pred):\n",
    "    assert Y.shape == Y_pred.shape\n",
    "    return np.sqrt(np.mean(np.square(Y_pred - Y)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Datasets #\n",
    "1. Loading Data\n",
    "2. Cleaning Datasets\n",
    "3. Splitting Data into Test/Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1482535, 8) (693359, 7)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_table('Dataset/train.tsv')\n",
    "test_df = pd.read_table('Dataset/test.tsv')\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing & Cleaning Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing low prices, anything below 3. Postinggs below 3 are likely to be an error. Removing them helps the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1481661, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove low prices\n",
    "train_df = train_df.drop(train_df[(train_df.price < 3.0)].index)\n",
    "train_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Stop-words from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>3</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>3</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>Razer</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>This keyboard great condition works like came ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>Target</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top hint lace key hole back! The pale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>1</td>\n",
       "      <td>Home/Home Décor/Home Décor Accents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>New tags. Leather horses. Retail [rm] each. St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Complete certificate authenticity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id                                 name  item_condition_id  \\\n",
       "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
       "1         1     Razer BlackWidow Chroma Keyboard                  3   \n",
       "2         2                       AVA-VIV Blouse                  1   \n",
       "3         3                Leather Horse Statues                  1   \n",
       "4         4                 24K GOLD plated rose                  1   \n",
       "\n",
       "                                       category_name brand_name  price  \\\n",
       "0                                  Men/Tops/T-shirts        NaN   10.0   \n",
       "1  Electronics/Computers & Tablets/Components & P...      Razer   52.0   \n",
       "2                        Women/Tops & Blouses/Blouse     Target   10.0   \n",
       "3                 Home/Home Décor/Home Décor Accents        NaN   35.0   \n",
       "4                            Women/Jewelry/Necklaces        NaN   44.0   \n",
       "\n",
       "   shipping                                   item_description  \n",
       "0         1                                 No description yet  \n",
       "1         0  This keyboard great condition works like came ...  \n",
       "2         1  Adorable top hint lace key hole back! The pale...  \n",
       "3         1  New tags. Leather horses. Retail [rm] each. St...  \n",
       "4         0                  Complete certificate authenticity  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "train_df[\"item_description\"].fillna(value='No description yet', inplace=True)\n",
    "train_df['item_description'] = train_df['item_description'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "train_df[\"name\"].fillna(value=\"missing\", inplace=True)\n",
    "train_df['name'] = train_df['name'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "test_df[\"item_description\"].fillna(value='No description yet', inplace=True)\n",
    "test_df['item_description'] = test_df['item_description'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "test_df[\"name\"].fillna(value=\"missing\", inplace=True)\n",
    "test_df['name'] = test_df['name'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the description, that is the raw number of words used, does have some correlation with price. The RNN might find this out on it's own, but since a max depth is used to save computations, it does not always know. Description length clearly helps the model, name length maybe not so much. Does not hurt the models so leaving name length in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "      <th>desc_len</th>\n",
       "      <th>name_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>3</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>3</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>Razer</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>This keyboard great condition works like came ...</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>Target</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top hint lace key hole back! The pale...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>1</td>\n",
       "      <td>Home/Home Décor/Home Décor Accents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>New tags. Leather horses. Retail [rm] each. St...</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Complete certificate authenticity</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id                                 name  item_condition_id  \\\n",
       "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
       "1         1     Razer BlackWidow Chroma Keyboard                  3   \n",
       "2         2                       AVA-VIV Blouse                  1   \n",
       "3         3                Leather Horse Statues                  1   \n",
       "4         4                 24K GOLD plated rose                  1   \n",
       "\n",
       "                                       category_name brand_name  price  \\\n",
       "0                                  Men/Tops/T-shirts        NaN   10.0   \n",
       "1  Electronics/Computers & Tablets/Components & P...      Razer   52.0   \n",
       "2                        Women/Tops & Blouses/Blouse     Target   10.0   \n",
       "3                 Home/Home Décor/Home Décor Accents        NaN   35.0   \n",
       "4                            Women/Jewelry/Necklaces        NaN   44.0   \n",
       "\n",
       "   shipping                                   item_description  desc_len  \\\n",
       "0         1                                 No description yet         0   \n",
       "1         0  This keyboard great condition works like came ...        21   \n",
       "2         1  Adorable top hint lace key hole back! The pale...        16   \n",
       "3         1  New tags. Leather horses. Retail [rm] each. St...        22   \n",
       "4         0                  Complete certificate authenticity         3   \n",
       "\n",
       "   name_len  \n",
       "0         7  \n",
       "1         4  \n",
       "2         2  \n",
       "3         3  \n",
       "4         4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get name and description lengths\n",
    "def wordCount(text):\n",
    "    try:\n",
    "        if text == 'No description yet':\n",
    "            return 0\n",
    "        else:\n",
    "            text = text.lower()\n",
    "            words = [w for w in text.split(\" \")]\n",
    "            return len(words)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "train_df['desc_len'] = train_df['item_description'].apply(lambda x: wordCount(x))\n",
    "test_df['desc_len'] = test_df['item_description'].apply(lambda x: wordCount(x))\n",
    "train_df['name_len'] = train_df['name'].apply(lambda x: wordCount(x))\n",
    "test_df['name_len'] = test_df['name'].apply(lambda x: wordCount(x))\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the category_name into 3 parts. The RNN models can get more information this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split category name into 3 parts\n",
    "def split_cat(text):\n",
    "    try:\n",
    "        return text.split(\"/\")\n",
    "    except:\n",
    "        return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "\n",
    "train_df['subcat_0'], train_df['subcat_1'], train_df['subcat_2'] = \\\n",
    "    zip(*train_df['category_name'].apply(lambda x: split_cat(x)))\n",
    "test_df['subcat_0'], test_df['subcat_1'], test_df['subcat_2'] = \\\n",
    "    zip(*test_df['category_name'].apply(lambda x: split_cat(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brand name data is sparse, missing over 600,000 values. This gets *some* of those values back by checking their names. An *exact* name match against all_brand names will help finding some of the missing values. We can be pretty confident in these. At the other extreme, we can search for *any* matches throughout all words in name. This finds over 200,000 but a lot of these are incorrect. Can land somewhere in the middle by either keeping cases or trimming out some of the 5000 brand names.\n",
    "\n",
    "For example, PINK is a brand by victoria secret. If we remove case, then almost all *pink* items are labeled as PINK brand. The other issue is that some of the \"brand names\" are not brands but really categories like \"Boots\" or \"Keys\". \n",
    "\n",
    "Currently, checking every word in name of a case-sensitive match does best. This gets around 137,000 finds while avoiding the problems with brands like PINK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137314\n"
     ]
    }
   ],
   "source": [
    "# attempt to find missing brand names\n",
    "# train_df['name'] = train_df.name.str.lower()\n",
    "# train_df['brand_name'] = train_df.brand_name.str.lower()\n",
    "# test_df['name'] = test_df.name.str.lower()\n",
    "# test_df['brand_name'] = test_df.brand_name.str.lower()\n",
    "full_set = pd.concat([train_df, test_df])\n",
    "all_brands = set(full_set['brand_name'].values)\n",
    "train_df.brand_name.fillna(value=\"missing\", inplace=True)\n",
    "test_df.brand_name.fillna(value=\"missing\", inplace=True)\n",
    "\n",
    "# get to finding!\n",
    "premissing = len(train_df.loc[train_df['brand_name'] == 'missing'])\n",
    "\n",
    "\n",
    "def brandfinder(line):\n",
    "    brand = line[0]\n",
    "    name = line[1]\n",
    "    namesplit = name.split(' ')\n",
    "    if brand == 'missing':\n",
    "        for x in namesplit:\n",
    "            if x in all_brands:\n",
    "                return name\n",
    "    if name in all_brands:\n",
    "        return name\n",
    "    return brand\n",
    "\n",
    "\n",
    "train_df['brand_name'] = train_df[['brand_name', 'name']].apply(brandfinder, axis=1)\n",
    "test_df['brand_name'] = test_df[['brand_name', 'name']].apply(brandfinder, axis=1)\n",
    "found = premissing-len(train_df.loc[train_df['brand_name'] == 'missing'])\n",
    "print(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1185328 examples\n",
      "Validating on 296333 examples\n",
      "Testing on 693359 examples\n"
     ]
    }
   ],
   "source": [
    "# Scale target variable to log.\n",
    "train_df[\"target\"] = np.log1p(train_df.price)\n",
    "\n",
    "# Split training examples into train/dev examples.\n",
    "train, test = train_test_split(train_df, random_state=123, train_size=0.8)\n",
    "\n",
    "# Calculate number of train/dev/test examples.\n",
    "n_trains = train.shape[0]\n",
    "n_devs = test.shape[0]\n",
    "n_tests = test_df.shape[0]\n",
    "print(\"Training on\", n_trains, \"examples\")\n",
    "print(\"Validating on\", n_devs, \"examples\")\n",
    "print(\"Testing on\", n_tests, \"examples\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model fitting:\n",
    "1. Preprocessing data\n",
    "2. Define RNN model\n",
    "3. Fitting RNN model on training examples\n",
    "4. Evaluating RNN model on dev examples\n",
    "5. Make prediction for test data using RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train - dev - test data for easy to handle\n",
    "full_df = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "      <th>desc_len</th>\n",
       "      <th>name_len</th>\n",
       "      <th>subcat_0</th>\n",
       "      <th>subcat_1</th>\n",
       "      <th>subcat_2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1388738</th>\n",
       "      <td>1388738</td>\n",
       "      <td>Bath Body Works aromatherapy lotion.</td>\n",
       "      <td>1</td>\n",
       "      <td>Beauty/Skin Care/Body</td>\n",
       "      <td>Bath &amp; Body Works</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bath Body Works aromatherapy body lotion. BRAN...</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>Skin Care</td>\n",
       "      <td>Body</td>\n",
       "      <td>2.484907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077160</th>\n",
       "      <td>1077160</td>\n",
       "      <td>Step Up 2&amp;3</td>\n",
       "      <td>1</td>\n",
       "      <td>Electronics/Media/Blu-Ray</td>\n",
       "      <td>Step Up 2&amp;3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Step Up 2 The Streets Step Up 3 Both new</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Media</td>\n",
       "      <td>Blu-Ray</td>\n",
       "      <td>2.833213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270135</th>\n",
       "      <td>1270135</td>\n",
       "      <td>Lululemon Tank</td>\n",
       "      <td>3</td>\n",
       "      <td>Sports &amp; Outdoors/Exercise/Athletic Training</td>\n",
       "      <td>Lululemon Tank</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Teal lulu top, back splits I cut long tag neck</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>Sports &amp; Outdoors</td>\n",
       "      <td>Exercise</td>\n",
       "      <td>Athletic Training</td>\n",
       "      <td>2.397895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834631</th>\n",
       "      <td>834631</td>\n",
       "      <td>ON HOLD FOR JLOVE</td>\n",
       "      <td>2</td>\n",
       "      <td>Vintage &amp; Collectibles/Clothing/Shorts</td>\n",
       "      <td>Maurices</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Jean shorts Maurice's, Jean shorts hollister, ...</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>Vintage &amp; Collectibles</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>Shorts</td>\n",
       "      <td>3.713572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762041</th>\n",
       "      <td>762041</td>\n",
       "      <td>Nike Shox</td>\n",
       "      <td>2</td>\n",
       "      <td>Women/Shoes/Athletic</td>\n",
       "      <td>Nike</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Women's 9, practically brand new worn like 2 t...</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>Women</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Athletic</td>\n",
       "      <td>3.931826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869112</th>\n",
       "      <td>869112</td>\n",
       "      <td>God, Goals, Grind Graphic Tee</td>\n",
       "      <td>2</td>\n",
       "      <td>Women/Tops &amp; Blouses/T-Shirts</td>\n",
       "      <td>missing</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>God, Goals, Grind Graphic Tee size medium. Wor...</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>Women</td>\n",
       "      <td>Tops &amp; Blouses</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>2.302585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889051</th>\n",
       "      <td>889051</td>\n",
       "      <td>Benefit \"fakeup\" concealer stick</td>\n",
       "      <td>3</td>\n",
       "      <td>Beauty/Makeup/Face</td>\n",
       "      <td>Benefit</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hydrating concealer color 01 light. Used sanit...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>Makeup</td>\n",
       "      <td>Face</td>\n",
       "      <td>2.197225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812733</th>\n",
       "      <td>812733</td>\n",
       "      <td>**Bundle HoneyLove**</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Athletic Apparel/Pants, Tights, Leggings</td>\n",
       "      <td>missing</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>BNWT Lularoe TC Leggings Bundle Mint purple Fl...</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>Women</td>\n",
       "      <td>Athletic Apparel</td>\n",
       "      <td>Pants, Tights, Leggings</td>\n",
       "      <td>4.110874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210108</th>\n",
       "      <td>1210108</td>\n",
       "      <td>Res!! M pink style lace choker bralette</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Other/Other</td>\n",
       "      <td>Res!! M pink style lace choker bralette</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand new Small-XL Black Wine red</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>Women</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>2.708050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670308</th>\n",
       "      <td>670308</td>\n",
       "      <td>4 Starbuck drink cards</td>\n",
       "      <td>1</td>\n",
       "      <td>Home/Kitchen &amp; Dining/Coffee &amp; Tea Accessories</td>\n",
       "      <td>missing</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Good 1 grande espresso beverage. All cards sti...</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>Home</td>\n",
       "      <td>Kitchen &amp; Dining</td>\n",
       "      <td>Coffee &amp; Tea Accessories</td>\n",
       "      <td>2.397895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1481661 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_id                                     name  item_condition_id  \\\n",
       "1388738   1388738     Bath Body Works aromatherapy lotion.                  1   \n",
       "1077160   1077160                              Step Up 2&3                  1   \n",
       "1270135   1270135                           Lululemon Tank                  3   \n",
       "834631     834631                        ON HOLD FOR JLOVE                  2   \n",
       "762041     762041                                Nike Shox                  2   \n",
       "...           ...                                      ...                ...   \n",
       "869112     869112            God, Goals, Grind Graphic Tee                  2   \n",
       "889051     889051         Benefit \"fakeup\" concealer stick                  3   \n",
       "812733     812733                     **Bundle HoneyLove**                  1   \n",
       "1210108   1210108  Res!! M pink style lace choker bralette                  1   \n",
       "670308     670308                   4 Starbuck drink cards                  1   \n",
       "\n",
       "                                          category_name  \\\n",
       "1388738                           Beauty/Skin Care/Body   \n",
       "1077160                       Electronics/Media/Blu-Ray   \n",
       "1270135    Sports & Outdoors/Exercise/Athletic Training   \n",
       "834631           Vintage & Collectibles/Clothing/Shorts   \n",
       "762041                             Women/Shoes/Athletic   \n",
       "...                                                 ...   \n",
       "869112                    Women/Tops & Blouses/T-Shirts   \n",
       "889051                               Beauty/Makeup/Face   \n",
       "812733   Women/Athletic Apparel/Pants, Tights, Leggings   \n",
       "1210108                               Women/Other/Other   \n",
       "670308   Home/Kitchen & Dining/Coffee & Tea Accessories   \n",
       "\n",
       "                                      brand_name  price  shipping  \\\n",
       "1388738                        Bath & Body Works   11.0         0   \n",
       "1077160                              Step Up 2&3   16.0         1   \n",
       "1270135                           Lululemon Tank   10.0         1   \n",
       "834631                                  Maurices   40.0         1   \n",
       "762041                                      Nike   50.0         1   \n",
       "...                                          ...    ...       ...   \n",
       "869112                                   missing    9.0         0   \n",
       "889051                                   Benefit    8.0         0   \n",
       "812733                                   missing   60.0         1   \n",
       "1210108  Res!! M pink style lace choker bralette   14.0         1   \n",
       "670308                                   missing   10.0         1   \n",
       "\n",
       "                                          item_description  desc_len  \\\n",
       "1388738  Bath Body Works aromatherapy body lotion. BRAN...        18   \n",
       "1077160           Step Up 2 The Streets Step Up 3 Both new        10   \n",
       "1270135     Teal lulu top, back splits I cut long tag neck        10   \n",
       "834631   Jean shorts Maurice's, Jean shorts hollister, ...        20   \n",
       "762041   Women's 9, practically brand new worn like 2 t...        14   \n",
       "...                                                    ...       ...   \n",
       "869112   God, Goals, Grind Graphic Tee size medium. Wor...        16   \n",
       "889051   Hydrating concealer color 01 light. Used sanit...         7   \n",
       "812733   BNWT Lularoe TC Leggings Bundle Mint purple Fl...        37   \n",
       "1210108                  Brand new Small-XL Black Wine red         6   \n",
       "670308   Good 1 grande espresso beverage. All cards sti...        16   \n",
       "\n",
       "         name_len                subcat_0          subcat_1  \\\n",
       "1388738         5                  Beauty         Skin Care   \n",
       "1077160         3             Electronics             Media   \n",
       "1270135         2       Sports & Outdoors          Exercise   \n",
       "834631          4  Vintage & Collectibles          Clothing   \n",
       "762041          2                   Women             Shoes   \n",
       "...           ...                     ...               ...   \n",
       "869112          5                   Women    Tops & Blouses   \n",
       "889051          4                  Beauty            Makeup   \n",
       "812733          2                   Women  Athletic Apparel   \n",
       "1210108         7                   Women             Other   \n",
       "670308          4                    Home  Kitchen & Dining   \n",
       "\n",
       "                         subcat_2    target  \n",
       "1388738                      Body  2.484907  \n",
       "1077160                   Blu-Ray  2.833213  \n",
       "1270135         Athletic Training  2.397895  \n",
       "834631                     Shorts  3.713572  \n",
       "762041                   Athletic  3.931826  \n",
       "...                           ...       ...  \n",
       "869112                   T-Shirts  2.302585  \n",
       "889051                       Face  2.197225  \n",
       "812733    Pants, Tights, Leggings  4.110874  \n",
       "1210108                     Other  2.708050  \n",
       "670308   Coffee & Tea Accessories  2.397895  \n",
       "\n",
       "[1481661 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing data\n",
    "Note that replacing 'No description yet' with \"missing\" helps the model a bit by treating it the same as the NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing data...\n",
      "1388738                             Beauty/Skin Care/Body\n",
      "1077160                         Electronics/Media/Blu-Ray\n",
      "1270135      Sports & Outdoors/Exercise/Athletic Training\n",
      "834631             Vintage & Collectibles/Clothing/Shorts\n",
      "762041                               Women/Shoes/Athletic\n",
      "                                ...                      \n",
      "869112                      Women/Tops & Blouses/T-Shirts\n",
      "889051                                 Beauty/Makeup/Face\n",
      "812733     Women/Athletic Apparel/Pants, Tights, Leggings\n",
      "1210108                                 Women/Other/Other\n",
      "670308     Home/Kitchen & Dining/Coffee & Tea Accessories\n",
      "Name: category_name, Length: 1481661, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Filling missing values\n",
    "def fill_missing_values(df):\n",
    "    df[\"category_name\"].fillna(value=\"missing\", inplace=True)\n",
    "    df[\"brand_name\"].fillna(value=\"missing\", inplace=True)\n",
    "    df[\"item_description\"].fillna(value=\"missing\", inplace=True)\n",
    "    df[\"item_description\"].replace('No description yet', \"missing\", inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Filling missing data...\")\n",
    "full_df = fill_missing_values(full_df)\n",
    "print(full_df.loc[:,\"category_name\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing categorical data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing categorical data...\")\n",
    "le = LabelEncoder()\n",
    "# full_df.category = full_df.category_name\n",
    "le.fit(full_df.category_name)\n",
    "full_df['category'] = le.transform(full_df.category_name)\n",
    "\n",
    "le.fit(full_df.brand_name)\n",
    "full_df.brand_name = le.transform(full_df.brand_name)\n",
    "\n",
    "le.fit(full_df.subcat_0)\n",
    "full_df.subcat_0 = le.transform(full_df.subcat_0)\n",
    "\n",
    "le.fit(full_df.subcat_1)\n",
    "full_df.subcat_1 = le.transform(full_df.subcat_1)\n",
    "\n",
    "le.fit(full_df.subcat_2)\n",
    "full_df.subcat_2 = le.transform(full_df.subcat_2)\n",
    "\n",
    "# del le"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Text Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming Text Data into Tokens & then Sequences of numbers would help in fitting the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming text data to sequences...\n",
      "Fitting tokenizer...\n",
      "Transforming text to sequences...\n"
     ]
    }
   ],
   "source": [
    "print(\"Transforming text data to sequences...\")\n",
    "raw_text = np.hstack([full_df[\"item_description\"].str.lower(), full_df[\"name\"].str.lower(), full_df[\"category_name\"].str.lower()])\n",
    "\n",
    "print(\"Fitting tokenizer...\")\n",
    "tok_raw = Tokenizer()\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "\n",
    "print(\"Transforming text to sequences...\")\n",
    "full_df['seq_item_description'] = tok_raw.texts_to_sequences(full_df[\"item_description\"].str.lower())\n",
    "full_df['seq_name'] = tok_raw.texts_to_sequences(full_df[\"name\"].str.lower())\n",
    "# full_df['seq_category'] = tok_raw.texts_to_sequences(full_df.category_name.str.lower())\n",
    "\n",
    "del tok_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bath body works aromatherapy body lotion. brand new full sized. sealed 6.5 fl oz. glass bottle brand new!!!',\n",
       "       'step up 2 the streets step up 3 both new',\n",
       "       'teal lulu top, back splits i cut long tag neck', ...,\n",
       "       'women/athletic apparel/pants, tights, leggings',\n",
       "       'women/other/other',\n",
       "       'home/kitchen & dining/coffee & tea accessories'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "      <th>desc_len</th>\n",
       "      <th>name_len</th>\n",
       "      <th>subcat_0</th>\n",
       "      <th>subcat_1</th>\n",
       "      <th>subcat_2</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "      <th>seq_item_description</th>\n",
       "      <th>seq_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1388738</th>\n",
       "      <td>1388738</td>\n",
       "      <td>Bath Body Works aromatherapy lotion.</td>\n",
       "      <td>1</td>\n",
       "      <td>Beauty/Skin Care/Body</td>\n",
       "      <td>15100</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bath Body Works aromatherapy body lotion. BRAN...</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>99</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>35</td>\n",
       "      <td>[229, 100, 221, 4458, 100, 619, 5, 2, 122, 105...</td>\n",
       "      <td>[229, 100, 221, 4458, 619]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077160</th>\n",
       "      <td>1077160</td>\n",
       "      <td>Step Up 2&amp;3</td>\n",
       "      <td>1</td>\n",
       "      <td>Electronics/Media/Blu-Ray</td>\n",
       "      <td>104785</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Step Up 2 The Streets Step Up 3 Both new</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>95</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>95</td>\n",
       "      <td>[1864, 222, 10, 38, 13226, 1864, 222, 23, 396, 2]</td>\n",
       "      <td>[1864, 222, 10, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270135</th>\n",
       "      <td>1270135</td>\n",
       "      <td>Lululemon Tank</td>\n",
       "      <td>3</td>\n",
       "      <td>Sports &amp; Outdoors/Exercise/Athletic Training</td>\n",
       "      <td>68376</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Teal lulu top, back splits I cut long tag neck</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "      <td>33</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>890</td>\n",
       "      <td>[690, 2568, 46, 101, 11505, 6, 303, 108, 202, ...</td>\n",
       "      <td>[315, 120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834631</th>\n",
       "      <td>834631</td>\n",
       "      <td>ON HOLD FOR JLOVE</td>\n",
       "      <td>2</td>\n",
       "      <td>Vintage &amp; Collectibles/Clothing/Shorts</td>\n",
       "      <td>71248</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Jean shorts Maurice's, Jean shorts hollister, ...</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>695</td>\n",
       "      <td>3.713572</td>\n",
       "      <td>1016</td>\n",
       "      <td>[614, 85, 3592, 614, 85, 549, 57, 431, 85, 311...</td>\n",
       "      <td>[153, 227, 91, 47751]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762041</th>\n",
       "      <td>762041</td>\n",
       "      <td>Nike Shox</td>\n",
       "      <td>2</td>\n",
       "      <td>Women/Shoes/Athletic</td>\n",
       "      <td>82094</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Women's 9, practically brand new worn like 2 t...</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>88</td>\n",
       "      <td>31</td>\n",
       "      <td>3.931826</td>\n",
       "      <td>1203</td>\n",
       "      <td>[35, 136, 2409, 5, 2, 19, 42, 10, 130, 81, 7, ...</td>\n",
       "      <td>[77, 5423]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_id                                  name  item_condition_id  \\\n",
       "1388738   1388738  Bath Body Works aromatherapy lotion.                  1   \n",
       "1077160   1077160                           Step Up 2&3                  1   \n",
       "1270135   1270135                        Lululemon Tank                  3   \n",
       "834631     834631                     ON HOLD FOR JLOVE                  2   \n",
       "762041     762041                             Nike Shox                  2   \n",
       "\n",
       "                                        category_name  brand_name  price  \\\n",
       "1388738                         Beauty/Skin Care/Body       15100   11.0   \n",
       "1077160                     Electronics/Media/Blu-Ray      104785   16.0   \n",
       "1270135  Sports & Outdoors/Exercise/Athletic Training       68376   10.0   \n",
       "834631         Vintage & Collectibles/Clothing/Shorts       71248   40.0   \n",
       "762041                           Women/Shoes/Athletic       82094   50.0   \n",
       "\n",
       "         shipping                                   item_description  \\\n",
       "1388738         0  Bath Body Works aromatherapy body lotion. BRAN...   \n",
       "1077160         1           Step Up 2 The Streets Step Up 3 Both new   \n",
       "1270135         1     Teal lulu top, back splits I cut long tag neck   \n",
       "834631          1  Jean shorts Maurice's, Jean shorts hollister, ...   \n",
       "762041          1  Women's 9, practically brand new worn like 2 t...   \n",
       "\n",
       "         desc_len  name_len  subcat_0  subcat_1  subcat_2    target  category  \\\n",
       "1388738        18         5         0        90        99  2.484907        35   \n",
       "1077160        10         3         1        65        95  2.833213        95   \n",
       "1270135        10         2         8        37        33  2.397895       890   \n",
       "834631         20         4         9        27       695  3.713572      1016   \n",
       "762041         14         2        10        88        31  3.931826      1203   \n",
       "\n",
       "                                      seq_item_description  \\\n",
       "1388738  [229, 100, 221, 4458, 100, 619, 5, 2, 122, 105...   \n",
       "1077160  [1864, 222, 10, 38, 13226, 1864, 222, 23, 396, 2]   \n",
       "1270135  [690, 2568, 46, 101, 11505, 6, 303, 108, 202, ...   \n",
       "834631   [614, 85, 3592, 614, 85, 549, 57, 431, 85, 311...   \n",
       "762041   [35, 136, 2409, 5, 2, 19, 42, 10, 130, 81, 7, ...   \n",
       "\n",
       "                           seq_name  \n",
       "1388738  [229, 100, 221, 4458, 619]  \n",
       "1077160         [1864, 222, 10, 23]  \n",
       "1270135                  [315, 120]  \n",
       "834631        [153, 227, 91, 47751]  \n",
       "762041                   [77, 5423]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(raw_text)\n",
    "display(full_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Define the Keras TensorBoard callback.logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "%load_ext tensorboard\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants to use when define RNN model\n",
    "Note the comments next to the first few lines indicate the longest entry in that column. Just for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NAME_SEQ = 10  # 17\n",
    "MAX_ITEM_DESC_SEQ = 75  # 269\n",
    "MAX_CATEGORY_SEQ = 8  # 8\n",
    "MAX_TEXT = np.max([\n",
    "    np.max(full_df[\"seq_name\"].max()),\n",
    "    np.max(full_df[\"seq_item_description\"].max()),\n",
    "    #     np.max(full_df.seq_category.max()),\n",
    "]) + 100\n",
    "MAX_CATEGORY = np.max(full_df[\"category\"].max()) + 1\n",
    "MAX_BRAND = np.max(full_df[\"brand_name\"].max()) + 1\n",
    "MAX_CONDITION = np.max(full_df[\"item_condition_id\"].max()) + 1\n",
    "MAX_DESC_LEN = np.max(full_df[\"desc_len\"].max()) + 1\n",
    "MAX_NAME_LEN = np.max(full_df[\"name_len\"].max()) + 1\n",
    "MAX_SUBCAT_0 = np.max(full_df[\"subcat_0\"].max()) + 1\n",
    "MAX_SUBCAT_1 = np.max(full_df[\"subcat_1\"].max()) + 1\n",
    "MAX_SUBCAT_2 = np.max(full_df[\"subcat_2\"].max()) + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data for RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the datasets from a Pandas dataframe to a dictionary to train the RNN models...\n",
    "\n",
    "def get_rnn_data(dataset):\n",
    "    X = {\n",
    "        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ),\n",
    "        'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ),\n",
    "        'brand_name': np.array(dataset.brand_name),\n",
    "        'category': np.array(dataset.category),\n",
    "        #         'category_name': pad_sequences(dataset.seq_category, maxlen=MAX_CATEGORY_SEQ),\n",
    "        'item_condition': np.array(dataset.item_condition_id),\n",
    "        'num_vars': np.array(dataset[[\"shipping\"]]),\n",
    "        'desc_len': np.array(dataset[[\"desc_len\"]]),\n",
    "        'name_len': np.array(dataset[[\"name_len\"]]),\n",
    "        'subcat_0': np.array(dataset.subcat_0),\n",
    "        'subcat_1': np.array(dataset.subcat_1),\n",
    "        'subcat_2': np.array(dataset.subcat_2),\n",
    "    }\n",
    "    return X\n",
    "\n",
    "\n",
    "train = full_df[:n_trains]\n",
    "test = full_df[n_trains:n_trains+n_devs]\n",
    "# test = full_df[n_trains+n_devs:]\n",
    "\n",
    "X_train = get_rnn_data(train)\n",
    "Y_train = train[\"target\"].values.reshape(-1, 1)\n",
    "\n",
    "X_test = get_rnn_data(test)\n",
    "Y_test = test[\"target\"].values.reshape(-1, 1)\n",
    "\n",
    "# X_test = get_rnn_data(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " brand_name (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " item_condition (InputLayer)    [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " desc_len (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " name_len (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " subcat_0 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " subcat_1 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " subcat_2 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " item_desc (InputLayer)         [(None, 75)]         0           []                               \n",
      "                                                                                                  \n",
      " name (InputLayer)              [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 10)        1258580     ['brand_name[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 5)         30          ['item_condition[0][0]']         \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 1, 5)         1190        ['desc_len[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 1, 5)         90          ['name_len[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, 1, 10)        110         ['subcat_0[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, 1, 10)        1140        ['subcat_1[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)        (None, 1, 10)        8710        ['subcat_2[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 75, 60)       15180720    ['item_desc[0][0]']              \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 10, 20)       5060240     ['name[0][0]']                   \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 10)           0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 5)            0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 5)            0           ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 5)            0           ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 10)           0           ['embedding_6[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 10)           0           ['embedding_7[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 10)           0           ['embedding_8[0][0]']            \n",
      "                                                                                                  \n",
      " gru (GRU)                      (None, 16)           3744        ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 8)            720         ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " num_vars (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 80)           0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]',              \n",
      "                                                                  'flatten_3[0][0]',              \n",
      "                                                                  'flatten_4[0][0]',              \n",
      "                                                                  'flatten_5[0][0]',              \n",
      "                                                                  'flatten_6[0][0]',              \n",
      "                                                                  'gru[0][0]',                    \n",
      "                                                                  'gru_1[0][0]',                  \n",
      "                                                                  'num_vars[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          41472       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          131328      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 256)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          32896       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           8256        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64)           0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            65          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,729,291\n",
      "Trainable params: 21,729,291\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\keras\\optimizers\\legacy\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# set seed again in case testing models adjustments by looping next 2 blocks\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "def new_rnn_model(lr=0.001, decay=0.0):\n",
    "    # Inputs\n",
    "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n",
    "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    brand_name = Input(shape=[1], name=\"brand_name\")\n",
    "#     category = Input(shape=[1], name=\"category\")\n",
    "#     category_name = Input(shape=[X_train[\"category_name\"].shape[1]], name=\"category_name\")\n",
    "    item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    desc_len = Input(shape=[1], name=\"desc_len\")\n",
    "    name_len = Input(shape=[1], name=\"name_len\")\n",
    "    subcat_0 = Input(shape=[1], name=\"subcat_0\")\n",
    "    subcat_1 = Input(shape=[1], name=\"subcat_1\")\n",
    "    subcat_2 = Input(shape=[1], name=\"subcat_2\")\n",
    "\n",
    "    # Embeddings layers (adjust outputs to help model)\n",
    "    emb_name = Embedding(MAX_TEXT, 20)(name)\n",
    "    emb_item_desc = Embedding(MAX_TEXT, 60)(item_desc)\n",
    "    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n",
    "#     emb_category_name = Embedding(MAX_TEXT, 20)(category_name)\n",
    "#     emb_category = Embedding(MAX_CATEGORY, 10)(category)\n",
    "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n",
    "    emb_desc_len = Embedding(MAX_DESC_LEN, 5)(desc_len)\n",
    "    emb_name_len = Embedding(MAX_NAME_LEN, 5)(name_len)\n",
    "    emb_subcat_0 = Embedding(MAX_SUBCAT_0, 10)(subcat_0)\n",
    "    emb_subcat_1 = Embedding(MAX_SUBCAT_1, 10)(subcat_1)\n",
    "    emb_subcat_2 = Embedding(MAX_SUBCAT_2, 10)(subcat_2)\n",
    "\n",
    "    # rnn layers (GRUs are faster than LSTMs and speed is important here)\n",
    "    rnn_layer1 = GRU(16)(emb_item_desc)\n",
    "    rnn_layer2 = GRU(8)(emb_name)\n",
    "#     rnn_layer3 = GRU(8) (emb_category_name)\n",
    "\n",
    "    # main layers\n",
    "    main_l = concatenate([\n",
    "        Flatten()(emb_brand_name)  \n",
    "        # , Flatten() (emb_category)\n",
    "        , Flatten()(emb_item_condition)\n",
    "        , Flatten()(emb_desc_len)\n",
    "        , Flatten()(emb_name_len)\n",
    "        , Flatten()(emb_subcat_0)\n",
    "        , Flatten()(emb_subcat_1)\n",
    "        , Flatten()(emb_subcat_2)\n",
    "        , rnn_layer1\n",
    "        , rnn_layer2  \n",
    "        # , rnn_layer3\n",
    "        , num_vars\n",
    "    ])\n",
    "    # (incressing the nodes or adding layers does not effect the time quite as much as the rnn layers)\n",
    "    main_l = Dropout(0.1)(Dense(512, kernel_initializer='normal', activation='relu')(main_l))\n",
    "    main_l = Dropout(0.1)(Dense(256, kernel_initializer='normal', activation='relu')(main_l))\n",
    "    main_l = Dropout(0.1)(Dense(128, kernel_initializer='normal', activation='relu')(main_l))\n",
    "    main_l = Dropout(0.1)(Dense(64, kernel_initializer='normal', activation='relu')(main_l))\n",
    "\n",
    "    # the output layer.\n",
    "    output = Dense(1, activation=\"linear\")(main_l)\n",
    "\n",
    "    model = Model([name, item_desc, brand_name, item_condition, num_vars, desc_len, name_len, subcat_0, subcat_1, subcat_2], output)\n",
    "\n",
    "    optimizer = Adam(lr=lr, decay=decay)\n",
    "    # (mean squared error loss function works as well as custom functions)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = new_rnn_model()\n",
    "model.summary()\n",
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting RNN model to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\keras\\engine\\functional.py:639: UserWarning: Input dict contained keys ['category'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772/772 [==============================] - 326s 410ms/step - loss: 0.3291 - val_loss: 0.1989\n",
      "Epoch 2/2\n",
      "772/772 [==============================] - 318s 412ms/step - loss: 0.2035 - val_loss: 0.1931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x198b0a1e2d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set hyper parameters for the model.\n",
    "BATCH_SIZE = 512 * 3\n",
    "epochs = 2\n",
    "\n",
    "# Calculate learning rate decay.\n",
    "\n",
    "\n",
    "def exp_decay(init, fin, steps): return (init/fin)**(1/(steps-1)) - 1\n",
    "\n",
    "\n",
    "steps = int(len(X_train['name']) / BATCH_SIZE) * epochs\n",
    "lr_init, lr_fin = 0.005, 0.001\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "\n",
    "# Create model and fit it with training dataset.\n",
    "rnn_model = new_rnn_model(lr=lr_init, decay=lr_decay)\n",
    "rnn_model.fit(X_train, Y_train, epochs=epochs, batch_size=BATCH_SIZE, validation_data=(X_test, Y_test), verbose=1, callbacks=[tensorboard_callback],)\n",
    "\n",
    "#callbacks=[tensorboard_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating RNN Model on Train & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on validation data...\n",
      "153/153 [==============================] - 10s 62ms/step\n",
      " RMSLE error: 0.4477298881579948\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Accuracy on Test Dataset\n",
    "print(\"Evaluating the model on validation data...\")\n",
    "Y_test_preds_rnn = rnn_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "print(\" RMSLE error:\", rmsle(Y_test, Y_test_preds_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on validation data...\n",
      "612/612 [==============================] - 34s 55ms/step\n",
      " RMSLE error: 0.3799419746089792\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Accuracy on Training Dataset\n",
    "print(\"Evaluating the model on validation data...\")\n",
    "Y_train_preds_rnn = rnn_model.predict(X_train, batch_size=BATCH_SIZE)\n",
    "print(\" RMSLE error:\", rmsle(Y_train, Y_train_preds_rnn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
